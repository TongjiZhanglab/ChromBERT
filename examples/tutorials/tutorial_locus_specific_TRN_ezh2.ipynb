{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for locus specific TRN inference\n",
    "Inference of transcriptional regulatory networks (TRNs) at specific loci is complex and dynamic, posing significant challenges. However, ChromBERTs offer a solution for this intricate task. In this tutorial, we will guide you through the process of locus-specific TRN inference, using the example of the non-classical functions of EZH2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset\n",
    "To define classical and non-classical sites of EZH2 in human embryonic stem cells (hESC), we use the EZH2 peak dataset (GSM1003524) and the H3K27me3 peak dataset (GSM1498900). Specifically, we define classical EZH2 sites as those where EZH2 co-localizes with H3K27me3. Conversely, non-classical EZH2 sites are identified where EZH2 does not co-localize with H3K27me3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdongxu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import  os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # to selected gpu used \n",
    "\n",
    "import sys \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import chrombert\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "basedir = \"/home/yangdongxu/repos/ChromBERT_reorder/data\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3\t93470270\t93470880\tpeak7668\t2339\t.\t23.98034\t243.39191\t233.90086\t232\n",
      "chr2\t91477646\t91478694\tpeak6148\t1127\t.\t10.28381\t119.74835\t112.79007\t430\n",
      "chr16\t46390276\t46390857\tpeak4186\t1039\t.\t8.55891\t110.78978\t103.94416\t350\n"
     ]
    }
   ],
   "source": [
    "peak_ezh2 = os.path.join(basedir, \"demo\", \"ezh2\", \"hESC_GSM1003524_EZH2.bed\")\n",
    "!head -3 {peak_ezh2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr10\t100480580\t100483730\tpeak1145\t6.37280\n",
      "chr10\t100519337\t100521146\tpeak1146\t6.86372\n",
      "chr10\t100655029\t100656371\tpeak1147\t15.71588\n"
     ]
    }
   ],
   "source": [
    "peak_h3k27me3 = os.path.join(basedir, \"demo\", \"ezh2\", \"hESC_GSM1498900_H3K27me3.bed\")\n",
    "!head -3 {peak_h3k27me3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  chrom   start     end  build_region_index  label\n",
       " 0  chr1  870000  871000                 174   True\n",
       " 1  chr1  905000  906000                 204   True\n",
       " 2  chr1  923000  924000                 220   True\n",
       " 3  chr1  924000  925000                 221   True\n",
       " 4  chr1  925000  926000                 222   True,\n",
       "   chrom  start    end  build_region_index  label\n",
       " 0  chr1  10000  11000                   0  False\n",
       " 1  chr1  16000  17000                   1  False\n",
       " 2  chr1  17000  18000                   2  False\n",
       " 3  chr1  29000  30000                   3  False\n",
       " 4  chr1  30000  31000                   4  False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we first align peaks to our predifined bins\n",
    "from chrombert.scripts.chrombert_make_dataset import get_overlap\n",
    "ref_regions = os.path.join(basedir, \"config\", \"hg38_6k_1kb_region.bed\")\n",
    "df1 = get_overlap(\n",
    "    supervised = peak_ezh2, \n",
    "    regions = ref_regions,\n",
    "    no_filter = False,\n",
    ").assign(label = lambda df: df[\"label\"] > 0 )\n",
    "df2 = get_overlap(\n",
    "    supervised = peak_h3k27me3, \n",
    "    regions = ref_regions,\n",
    "    no_filter = True,\n",
    ").assign(label = lambda df: df[\"label\"] > 0 )\n",
    "df1.head(), df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>build_region_index</th>\n",
       "      <th>EZH2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>870000</td>\n",
       "      <td>871000</td>\n",
       "      <td>174</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>905000</td>\n",
       "      <td>906000</td>\n",
       "      <td>204</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>923000</td>\n",
       "      <td>924000</td>\n",
       "      <td>220</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>924000</td>\n",
       "      <td>925000</td>\n",
       "      <td>221</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>925000</td>\n",
       "      <td>926000</td>\n",
       "      <td>222</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11003</th>\n",
       "      <td>chrX</td>\n",
       "      <td>154750000</td>\n",
       "      <td>154751000</td>\n",
       "      <td>2134828</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11004</th>\n",
       "      <td>chrY</td>\n",
       "      <td>5001000</td>\n",
       "      <td>5002000</td>\n",
       "      <td>2135730</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11005</th>\n",
       "      <td>chrY</td>\n",
       "      <td>10994000</td>\n",
       "      <td>10995000</td>\n",
       "      <td>2136403</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11006</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26670000</td>\n",
       "      <td>26671000</td>\n",
       "      <td>2137888</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26671000</td>\n",
       "      <td>26672000</td>\n",
       "      <td>2137889</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11008 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chrom      start        end  build_region_index  EZH2  label\n",
       "0      chr1     870000     871000                 174  True   True\n",
       "1      chr1     905000     906000                 204  True   True\n",
       "2      chr1     923000     924000                 220  True   True\n",
       "3      chr1     924000     925000                 221  True  False\n",
       "4      chr1     925000     926000                 222  True   True\n",
       "...     ...        ...        ...                 ...   ...    ...\n",
       "11003  chrX  154750000  154751000             2134828  True   True\n",
       "11004  chrY    5001000    5002000             2135730  True  False\n",
       "11005  chrY   10994000   10995000             2136403  True  False\n",
       "11006  chrY   26670000   26671000             2137888  True  False\n",
       "11007  chrY   26671000   26672000             2137889  True  False\n",
       "\n",
       "[11008 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_supervised = df1.rename(columns = {\"label\": \"EZH2\"}).merge(df2).assign(label = lambda df: df[\"label\"])\n",
    "df_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    5272\n",
       "True     5736\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_supervised.groupby(\"label\").size() # that's a near balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8806, 1101, 1101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we then split the dataset into training, validation and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_temp = train_test_split(df_supervised, test_size=0.2, random_state=42, stratify = df_supervised['label'])\n",
    "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify = df_temp['label'])\n",
    "\n",
    "os.makedirs(\"tmp_ezh2\", exist_ok=True)\n",
    "df_train.to_csv(os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "df_valid.to_csv(os.path.join(\"tmp_ezh2\", \"valid.csv\"))\n",
    "df_test.to_csv(os.path.join(\"tmp_ezh2\", \"test.csv\"))\n",
    "\n",
    "len(df_train), len(df_valid), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n",
    "In this section, we provide a tutorial for fine-tuning ChromBERTs to adapt to our specific task. The steps are similar to the original ChromBERTs process, with a few key modifications:\n",
    "\n",
    "- Dataset Preparation: We use the ignore_object parameter to exclude H3K27me3-related cistromes from the original ChromBERTs dataset, preventing interference from H3K27me3.\n",
    "- Model Instantiation: We introduce a special ignore_index parameter, derived from the dataset, to properly instantiate the model.  \n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intruction for dataset ignore given factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6391])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = chrombert.get_preset_dataset_config(\n",
    "    \"general\", \n",
    "    supervised_file = None, \n",
    "    ignore = False, ignore_object = \"h3k27me3\" # turn off ignore\n",
    "    )\n",
    "ds = dc.init_dataset(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "ds[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6185])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = chrombert.get_preset_dataset_config(\n",
    "    \"general\", \n",
    "    supervised_file = None, \n",
    "    ignore = True, ignore_object = \"h3k27me3\" # we ignore the h3k27me3 related cistrome, to avoid data leakage\n",
    "    )\n",
    "ds = dc.init_dataset(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "ignore_index = ds[0][\"ignore_index\"] # to get ignore_index used to build model. Currently, we only support same ignore object in one dataset, so we can get ignore_index from any sample. \n",
    "ds[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the dataset, after ignoring the specified cistromes, works as normal. However, the length of the input sequence is 6185 instead of 6391. A total of 206 H3K27me3-related cistromes are ignored and do not participate in the training process.\n",
    "\n",
    "Additionally, a small note: we tune the model using PyTorch Lightning, so the dataset is wrapped in the `lightning.pytorch.LightningDataModule` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chrombert.finetune.dataset.data_module.LitChromBERTFTDataModule at 0x7f79676db460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = chrombert.LitChromBERTFTDataModule(\n",
    "    config = dc,\n",
    "    train_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\")),\n",
    "    val_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"valid.csv\")),\n",
    "    test_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"test.csv\")),\n",
    ")\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Model\n",
    "Next, we can instantiate the model using the ignore_index parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: mtx_mask = config/hg38_6k_mask_matrix.tsv\n",
      "update path: pretrain_ckpt = checkpoint/hg38_6k_1kb_pretrain.ckpt\n",
      "use organisim hg38; max sequence length including cls is 6392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTGeneral                                        --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               (4,916,736)\n",
       "â”‚    â””â”€ModuleList: 2-2                                  51,978,240\n",
       "â”œâ”€GeneralHeader: 1-2                                    --\n",
       "â”‚    â””â”€CistromeEmbeddingManager: 2-3                    --\n",
       "â”‚    â””â”€Conv2d: 2-4                                      769\n",
       "â”‚    â””â”€ReLU: 2-5                                        --\n",
       "â”‚    â””â”€ResidualBlock: 2-6                               3,230,720\n",
       "â”‚    â””â”€ResidualBlock: 2-7                               2,166,528\n",
       "â”‚    â””â”€ResidualBlock: 2-8                               460,032\n",
       "â”‚    â””â”€Linear: 2-9                                      257\n",
       "================================================================================\n",
       "Total params: 62,753,282\n",
       "Trainable params: 18,852,866\n",
       "Non-trainable params: 43,900,416\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = chrombert.get_preset_model_config(\n",
    "    \"general\", \n",
    "    ignore = True, ignore_index =ignore_index  # ignore_index from above\n",
    ").init_model()\n",
    "model.freeze_pretrain(trainable=2) # we only fine-tune the last two layers\n",
    "summary(model, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n",
    "We fine-tune the model using PyTorch Lightning. A simple configuration is created to process parameters, and tuning is performed on a limited dataset to save time.\n",
    "\n",
    "Note: The tuning process is random, so results may vary. To achieve the best results, consider increasing the number of epochs and the size of the dataset used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chrombert.finetune.train.pl_module.ClassificationPLModule"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = chrombert.finetune.train.TrainConfig(\n",
    "    kind = \"classification\",\n",
    "    loss = \"bce\", # use bce loss because it's a balanced binary classification task\n",
    "    max_epochs = 1,\n",
    "    lr = 1e-4\n",
    ")\n",
    "pl_module = tc.init_pl_module(model) # wrap model with PyTorch Lightning module\n",
    "type(pl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we start tuning!   \n",
    "The trainer will save logs in a format supported by TensorBoard, and several checkpoints may be saved during the process.  \n",
    "However, in this tutorial, we use the latest model parameters instead of checkpoints due to limited tuning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | ChromBERTGeneral | 62.8 M\n",
      "-------------------------------------------\n",
      "18.9 M    Trainable params\n",
      "43.9 M    Non-trainable params\n",
      "62.8 M    Total params\n",
      "251.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3263fd7ab8264e808948e347d0d760c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:480: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530f749783ab4aab8b2d610d4d791d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2ca50cef75497e93896da0f6290bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438e3518650743c6970f5dc5a242f534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2d5d8d78eb432fb5339c4b3ecbb90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0724c4fc0a654c609912e10ce424a103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b185099e32467188c174d58f722a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fccb30ff674141b3a02b6a11a476b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20438b73362043d5bbf5375d5a1ccccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140e36c897eb4ea4973e26c61dfd4600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" # for tensorboard compatibility\n",
    "callback_ckpt = pl.callbacks.ModelCheckpoint( monitor = f\"{tc.tag}_validation/{tc.loss}\", mode = \"min\")\n",
    "# \n",
    "# \n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = tc.max_epochs,\n",
    "    accelerator = \"gpu\",\n",
    "    precision = \"bf16-mixed\",\n",
    "    fast_dev_run = False,\n",
    "    accumulate_grad_batches = 16, \n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.join(\"tmp_ezh2\",\"logs\"), name = \"ezh2\"),\n",
    "    val_check_interval = 128,\n",
    "    limit_val_batches = 128,\n",
    "    log_every_n_steps = 1,\n",
    "    callbacks = [ callback_ckpt, pl.callbacks.LearningRateMonitor() ],\n",
    ")\n",
    "trainer.fit(pl_module, data_module)\n",
    "pl_module.save_ckpt(os.path.join(\"tmp_ezh2\", \"ezh2.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tuned to get regulator embedding\n",
    "\n",
    "ChromBERT is now fine-tuned! You can access the tuned model directly using `pl_module.model`. However, please note that due to specific settings in flash-attention, you cannot change the dropout probability, which may introduce some randomness in the output.\n",
    "\n",
    "For consistent results, we recommend saving the checkpoint and loading it with the original model to ensure you have the tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: mtx_mask = config/hg38_6k_mask_matrix.tsv\n",
      "update path: pretrain_ckpt = checkpoint/hg38_6k_1kb_pretrain.ckpt\n",
      "update path: finetune_ckpt = /home/yangdongxu/repos/ChromBERT_reorder/examples/tutorials/tmp_ezh2/ezh2.ckpt\n",
      "use organisim hg38; max sequence length including cls is 6392\n",
      "Loading checkpoint from /home/yangdongxu/repos/ChromBERT_reorder/examples/tutorials/tmp_ezh2/ezh2.ckpt\n",
      "Loaded 110/110 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTGeneral                                        --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               4,916,736\n",
       "â”‚    â””â”€ModuleList: 2-2                                  51,978,240\n",
       "â”œâ”€GeneralHeader: 1-2                                    --\n",
       "â”‚    â””â”€CistromeEmbeddingManager: 2-3                    --\n",
       "â”‚    â””â”€Conv2d: 2-4                                      769\n",
       "â”‚    â””â”€ReLU: 2-5                                        --\n",
       "â”‚    â””â”€ResidualBlock: 2-6                               3,230,720\n",
       "â”‚    â””â”€ResidualBlock: 2-7                               2,166,528\n",
       "â”‚    â””â”€ResidualBlock: 2-8                               460,032\n",
       "â”‚    â””â”€Linear: 2-9                                      257\n",
       "================================================================================\n",
       "Total params: 62,753,282\n",
       "Trainable params: 62,753,282\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tuned = chrombert.get_preset_model_config(\n",
    "    \"general\", \n",
    "    dropout = 0,\n",
    "    ignore = True, ignore_index =ignore_index,   # ignore_index from above\n",
    "    finetune_ckpt = os.path.abspath(os.path.join(\"tmp_ezh2\", \"ezh2.ckpt\")) # use absolute path here, to avoid mixing of preset\n",
    ").init_model()\n",
    "# or use model_tuned = pl_module.model\n",
    "summary(model_tuned, depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get the embedding manager following the instruction of tutorials about extracting embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTEmbedding                                      --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               --\n",
       "â”‚    â”‚    â””â”€TokenEmbedding: 3-1                         7,680\n",
       "â”‚    â”‚    â””â”€PositionalEmbedding: 3-2                    4,909,056\n",
       "â”‚    â”‚    â””â”€Dropout: 3-3                                --\n",
       "â”‚    â””â”€ModuleList: 2-2                                  --\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-4                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-5                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-6                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-7                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-8                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-9                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-10               6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-11               6,497,280\n",
       "â”œâ”€CistromeEmbeddingManager: 1-2                         --\n",
       "================================================================================\n",
       "Total params: 56,894,976\n",
       "Trainable params: 56,894,976\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = model_tuned.get_embedding_manager().cuda()\n",
    "summary(model_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101,\n",
       " ['input_ids',\n",
       "  'position_ids',\n",
       "  'region',\n",
       "  'build_region_index',\n",
       "  'ignore_index',\n",
       "  'label'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_test = data_module.test_config\n",
    "ds_test = dc_test.init_dataset()\n",
    "dl_test = dc_test.init_dataloader(batch_size = 1)\n",
    "len(ds_test), list(ds_test[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1101/1101 [01:06<00:00, 16.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573 528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1063, 768), (1063, 768))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_classicial = []\n",
    "embs_nonclassicial = []\n",
    "for batch in tqdm(dl_test):\n",
    "    with torch.no_grad():\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = v.cuda()\n",
    "        emb = model_emb(batch)\n",
    "    if batch[\"label\"].item() == 1:\n",
    "        embs_classicial.append(emb)\n",
    "    else:\n",
    "        embs_nonclassicial.append(emb)\n",
    "\n",
    "print(len(embs_classicial), len(embs_nonclassicial))\n",
    "embs_classicial = torch.cat(embs_classicial, dim = 0).cpu().numpy().mean(axis = 0)\n",
    "embs_nonclassicial = torch.cat(embs_nonclassicial, dim = 0).cpu().numpy().mean(axis = 0)\n",
    "embs_classicial.shape, embs_nonclassicial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider only factors below, remove histone modifications and chromatin accessibility. Then we can get similarity between factors, which represent the potential interactions between factors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['adnp', 'aebp2', 'aff1'], 992)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(basedir, \"config\",\"hg38_6k_factors_list.txt\"),\"r\") as f:\n",
    "    factors = f.read().strip().split(\"\\n\")\n",
    "factors = [f.strip().lower() for f in factors]\n",
    "factors[:3], len(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.in1d(model_emb.list_regulator,factors)\n",
    "names = np.array(model_emb.list_regulator)[indices]\n",
    "embs_classicial = embs_classicial[indices]\n",
    "embs_nonclassicial = embs_nonclassicial[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_classicial_matrix = cosine_similarity(embs_classicial)\n",
    "cos_nonclassicial_matrix = cosine_similarity(embs_nonclassicial)\n",
    "df_cos_classicial = pd.DataFrame(cos_classicial_matrix, columns = names, index = names)\n",
    "df_cos_nonclassicial = pd.DataFrame(cos_nonclassicial_matrix, columns = names, index = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6016223430633545, 0.5621198415756226)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define threshold to select the most related regulators pairs\n",
    "thre_class = np.percentile(cos_classicial_matrix.flatten(), 95)\n",
    "thre_nonclass = np.percentile(cos_nonclassicial_matrix.flatten(), 95)\n",
    "thre_class, thre_nonclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we find TRNs related to EZH2 non-classical functions. As you see, factors related to EZH2 classical functions is polycomb complex related, e.g. SUZ12. However, factors related to EZH2 non-classical functions tends to be associated with transcriptional activation, e.g. EP300, TP53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adnp</th>\n",
       "      <td>0.270390</td>\n",
       "      <td>0.388505</td>\n",
       "      <td>-0.118115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aebp2</th>\n",
       "      <td>0.232409</td>\n",
       "      <td>0.334547</td>\n",
       "      <td>-0.102139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aff1</th>\n",
       "      <td>0.447353</td>\n",
       "      <td>0.485739</td>\n",
       "      <td>-0.038386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aff4</th>\n",
       "      <td>0.403353</td>\n",
       "      <td>0.492727</td>\n",
       "      <td>-0.089375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago1</th>\n",
       "      <td>0.282023</td>\n",
       "      <td>0.288040</td>\n",
       "      <td>-0.006017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zscan5a</th>\n",
       "      <td>0.268473</td>\n",
       "      <td>0.325382</td>\n",
       "      <td>-0.056909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zta</th>\n",
       "      <td>0.257478</td>\n",
       "      <td>0.291565</td>\n",
       "      <td>-0.034087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zxdb</th>\n",
       "      <td>0.250267</td>\n",
       "      <td>0.270615</td>\n",
       "      <td>-0.020347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zxdc</th>\n",
       "      <td>0.218339</td>\n",
       "      <td>0.287684</td>\n",
       "      <td>-0.069345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzz3</th>\n",
       "      <td>0.320594</td>\n",
       "      <td>0.377420</td>\n",
       "      <td>-0.056825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         classical  nonclassical      diff\n",
       "adnp      0.270390      0.388505 -0.118115\n",
       "aebp2     0.232409      0.334547 -0.102139\n",
       "aff1      0.447353      0.485739 -0.038386\n",
       "aff4      0.403353      0.492727 -0.089375\n",
       "ago1      0.282023      0.288040 -0.006017\n",
       "...            ...           ...       ...\n",
       "zscan5a   0.268473      0.325382 -0.056909\n",
       "zta       0.257478      0.291565 -0.034087\n",
       "zxdb      0.250267      0.270615 -0.020347\n",
       "zxdc      0.218339      0.287684 -0.069345\n",
       "zzz3      0.320594      0.377420 -0.056825\n",
       "\n",
       "[992 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2 = pd.DataFrame(index =names, data = {\"classical\":df_cos_classicial.loc[\"ezh2\",:],\"nonclassical\":df_cos_nonclassicial.loc[\"ezh2\",:]})\n",
    "df_cos_ezh2[\"diff\"] = df_cos_ezh2[\"classical\"] - df_cos_ezh2[\"nonclassical\"]\n",
    "df_cos_ezh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ezh1</th>\n",
       "      <td>0.627185</td>\n",
       "      <td>0.576553</td>\n",
       "      <td>5.063266e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suz12</th>\n",
       "      <td>0.877764</td>\n",
       "      <td>0.832076</td>\n",
       "      <td>4.568756e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bcor</th>\n",
       "      <td>0.602734</td>\n",
       "      <td>0.560390</td>\n",
       "      <td>4.234409e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jarid2</th>\n",
       "      <td>0.612120</td>\n",
       "      <td>0.588507</td>\n",
       "      <td>2.361345e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ezh2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.384186e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rnf2</th>\n",
       "      <td>0.704727</td>\n",
       "      <td>0.709211</td>\n",
       "      <td>-4.483521e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        classical  nonclassical          diff\n",
       "ezh1     0.627185      0.576553  5.063266e-02\n",
       "suz12    0.877764      0.832076  4.568756e-02\n",
       "bcor     0.602734      0.560390  4.234409e-02\n",
       "jarid2   0.612120      0.588507  2.361345e-02\n",
       "ezh2     1.000000      1.000000 -2.384186e-07\n",
       "rnf2     0.704727      0.709211 -4.483521e-03"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2.query(\"classical > @thre_class \").sort_values(\"diff\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>foxm1</th>\n",
       "      <td>0.450502</td>\n",
       "      <td>0.606847</td>\n",
       "      <td>-0.156345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hinfp</th>\n",
       "      <td>0.446726</td>\n",
       "      <td>0.583847</td>\n",
       "      <td>-0.137121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>med1</th>\n",
       "      <td>0.440611</td>\n",
       "      <td>0.574923</td>\n",
       "      <td>-0.134312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brca1</th>\n",
       "      <td>0.465605</td>\n",
       "      <td>0.587078</td>\n",
       "      <td>-0.121473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rela</th>\n",
       "      <td>0.447773</td>\n",
       "      <td>0.568249</td>\n",
       "      <td>-0.120476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat3</th>\n",
       "      <td>0.503793</td>\n",
       "      <td>0.620120</td>\n",
       "      <td>-0.116327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep300</th>\n",
       "      <td>0.477905</td>\n",
       "      <td>0.591606</td>\n",
       "      <td>-0.113701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tp53</th>\n",
       "      <td>0.465553</td>\n",
       "      <td>0.578920</td>\n",
       "      <td>-0.113366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e2f4</th>\n",
       "      <td>0.450022</td>\n",
       "      <td>0.562298</td>\n",
       "      <td>-0.112277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat1</th>\n",
       "      <td>0.469702</td>\n",
       "      <td>0.570898</td>\n",
       "      <td>-0.101195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       classical  nonclassical      diff\n",
       "foxm1   0.450502      0.606847 -0.156345\n",
       "hinfp   0.446726      0.583847 -0.137121\n",
       "med1    0.440611      0.574923 -0.134312\n",
       "brca1   0.465605      0.587078 -0.121473\n",
       "rela    0.447773      0.568249 -0.120476\n",
       "stat3   0.503793      0.620120 -0.116327\n",
       "ep300   0.477905      0.591606 -0.113701\n",
       "tp53    0.465553      0.578920 -0.113366\n",
       "e2f4    0.450022      0.562298 -0.112277\n",
       "stat1   0.469702      0.570898 -0.101195"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2.query(\"nonclassical > @thre_nonclass \").sort_values(\"diff\", ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "This tutorial provides a comprehensive guide to locus-specific TRN inference using EZH2' co-association as exmaple.   \n",
    "We hope you find it helpful and informative.   \n",
    "If you have any questions or need further assistance, please feel free to ask. Thank you for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
