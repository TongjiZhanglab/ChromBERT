{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for context-specific TRN: functional collaborations with EZH2 on funtional distinct loci\n",
    "Inference of transcriptional regulatory networks (TRNs) at specific loci is a complex and dynamic process. In this tutorial, we will guide you through context-specific TRN analysis using ChromBERTs, with EZH2 serving as an example of functional collaborations at distinct loci. \n",
    "\n",
    "**Attention: You should go through this [ tutorial ](https://chrombert.readthedocs.io/en/latest/tutorial_finetuning_ChromBERT.html) at first to get familiar with the basic usage of ChromBERT.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset\n",
    "\n",
    "To identify classical and non-classical EZH2 sites in human embryonic stem cells (hESCs), we utilize the EZH2 peak dataset (GSM1003524) and the H3K27me3 peak dataset (GSM1498900). Classical EZH2 sites are defined as regions where EZH2 co-localizes with H3K27me3, while non-classical EZH2 sites are identified as regions where EZH2 is present without H3K27me3 co-localization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yangdongxu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import  os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # to selected gpu used \n",
    "\n",
    "import sys \n",
    "import torch \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import chrombert\n",
    "from torchinfo import summary\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "basedir =  os.path.expanduser(\"~/.cache/chrombert/data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3\t93470270\t93470880\tpeak7668\t2339\t.\t23.98034\t243.39191\t233.90086\t232\n",
      "chr2\t91477646\t91478694\tpeak6148\t1127\t.\t10.28381\t119.74835\t112.79007\t430\n",
      "chr16\t46390276\t46390857\tpeak4186\t1039\t.\t8.55891\t110.78978\t103.94416\t350\n"
     ]
    }
   ],
   "source": [
    "peak_ezh2 = os.path.join(basedir, \"demo\", \"ezh2\", \"hESC_GSM1003524_EZH2.bed\")\n",
    "!head -3 {peak_ezh2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr10\t100480580\t100483730\tpeak1145\t6.37280\n",
      "chr10\t100519337\t100521146\tpeak1146\t6.86372\n",
      "chr10\t100655029\t100656371\tpeak1147\t15.71588\n"
     ]
    }
   ],
   "source": [
    "peak_h3k27me3 = os.path.join(basedir, \"demo\", \"ezh2\", \"hESC_GSM1498900_H3K27me3.bed\")\n",
    "!head -3 {peak_h3k27me3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(  chrom   start     end  build_region_index  label\n",
       " 0  chr1  870000  871000                 174   True\n",
       " 1  chr1  905000  906000                 204   True\n",
       " 2  chr1  923000  924000                 220   True\n",
       " 3  chr1  924000  925000                 221   True\n",
       " 4  chr1  925000  926000                 222   True,\n",
       "   chrom  start    end  build_region_index  label\n",
       " 0  chr1  10000  11000                   0  False\n",
       " 1  chr1  16000  17000                   1  False\n",
       " 2  chr1  17000  18000                   2  False\n",
       " 3  chr1  29000  30000                   3  False\n",
       " 4  chr1  30000  31000                   4  False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Align genomic coordinates from the narrowPeak file to the Human-Cistrome-6k dataset regions\n",
    "from chrombert.scripts.chrombert_make_dataset import get_overlap\n",
    "ref_regions = os.path.join(basedir, \"config\", \"hg38_6k_1kb_region.bed\")\n",
    "df1 = get_overlap(\n",
    "    supervised = peak_ezh2, \n",
    "    regions = ref_regions,\n",
    "    no_filter = False,\n",
    ").assign(label = lambda df: df[\"label\"] > 0 )\n",
    "df2 = get_overlap(\n",
    "    supervised = peak_h3k27me3, \n",
    "    regions = ref_regions,\n",
    "    no_filter = True,\n",
    ").assign(label = lambda df: df[\"label\"] > 0 )\n",
    "df1.head(), df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrom</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>build_region_index</th>\n",
       "      <th>EZH2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>870000</td>\n",
       "      <td>871000</td>\n",
       "      <td>174</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>905000</td>\n",
       "      <td>906000</td>\n",
       "      <td>204</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>923000</td>\n",
       "      <td>924000</td>\n",
       "      <td>220</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>924000</td>\n",
       "      <td>925000</td>\n",
       "      <td>221</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>925000</td>\n",
       "      <td>926000</td>\n",
       "      <td>222</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11003</th>\n",
       "      <td>chrX</td>\n",
       "      <td>154750000</td>\n",
       "      <td>154751000</td>\n",
       "      <td>2134828</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11004</th>\n",
       "      <td>chrY</td>\n",
       "      <td>5001000</td>\n",
       "      <td>5002000</td>\n",
       "      <td>2135730</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11005</th>\n",
       "      <td>chrY</td>\n",
       "      <td>10994000</td>\n",
       "      <td>10995000</td>\n",
       "      <td>2136403</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11006</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26670000</td>\n",
       "      <td>26671000</td>\n",
       "      <td>2137888</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11007</th>\n",
       "      <td>chrY</td>\n",
       "      <td>26671000</td>\n",
       "      <td>26672000</td>\n",
       "      <td>2137889</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11008 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chrom      start        end  build_region_index  EZH2  label\n",
       "0      chr1     870000     871000                 174  True   True\n",
       "1      chr1     905000     906000                 204  True   True\n",
       "2      chr1     923000     924000                 220  True   True\n",
       "3      chr1     924000     925000                 221  True  False\n",
       "4      chr1     925000     926000                 222  True   True\n",
       "...     ...        ...        ...                 ...   ...    ...\n",
       "11003  chrX  154750000  154751000             2134828  True   True\n",
       "11004  chrY    5001000    5002000             2135730  True  False\n",
       "11005  chrY   10994000   10995000             2136403  True  False\n",
       "11006  chrY   26670000   26671000             2137888  True  False\n",
       "11007  chrY   26671000   26672000             2137889  True  False\n",
       "\n",
       "[11008 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_supervised = df1.rename(columns = {\"label\": \"EZH2\"}).merge(df2).assign(label = lambda df: df[\"label\"])\n",
    "df_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    5272\n",
       "True     5736\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_supervised.groupby(\"label\").size() # that's a near balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8806, 1101, 1101)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we split the dataset into training, validation and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_temp = train_test_split(df_supervised, test_size=0.2, random_state=42, stratify = df_supervised['label'])\n",
    "df_valid, df_test = train_test_split(df_temp, test_size=0.5, random_state=42, stratify = df_temp['label'])\n",
    "\n",
    "os.makedirs(\"tmp_ezh2\", exist_ok=True)\n",
    "df_train.to_csv(os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "df_valid.to_csv(os.path.join(\"tmp_ezh2\", \"valid.csv\"))\n",
    "df_test.to_csv(os.path.join(\"tmp_ezh2\", \"test.csv\"))\n",
    "\n",
    "len(df_train), len(df_valid), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune\n",
    "\n",
    "In this section, we provide a tutorial on fine-tuning ChromBERTs for our specific task. The process closely follows the original ChromBERTs workflow, with a few important modifications:\n",
    "\n",
    "- **Dataset Preparation**: The `ignore_object` parameter is used to omit H3K27me3-related cistromes from the original ChromBERTs dataset, ensuring H3K27me3 does not interfere with the analysis.  \n",
    "- **Model Instantiation**: A special `ignore_index` parameter, derived from the dataset, is introduced to properly configure the model.  \n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for dataset: omit specified regulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6391])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc = chrombert.get_preset_dataset_config(\n",
    "    \"general\", \n",
    "    supervised_file = None, \n",
    "    ignore = False, ignore_object = \"h3k27me3\" # turn off omission\n",
    "    )\n",
    "ds = dc.init_dataset(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "ds[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: hdf5_file = hg38_6k_1kb.hdf5\n",
      "update path: meta_file = config/hg38_6k_meta.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6188])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We omit the h3k27me3 related cistrome, to avoid data leakage\n",
    "dc = chrombert.get_preset_dataset_config(\n",
    "    \"general\", \n",
    "    supervised_file = None, \n",
    "    ignore = True, ignore_object = \"h3k27me3\" \n",
    "    )\n",
    "ds = dc.init_dataset(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\"))\n",
    "\n",
    "# Get ignore_index used to instantiate model. \n",
    "# Currently, we only support same ignore object in one dataset, \n",
    "# so it's ok to get ignore_index from any sample. \n",
    "ignore_index = ds[0][\"ignore_index\"] \n",
    "ds[1][\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the dataset functions as expected after omitting the specified cistromes. However, the input sequence length is reduced to 6185 from 6391, as 206 H3K27me3-related cistromes are omitted and do not participate in the training process.\n",
    "\n",
    "A small note: the model is fine-tuned using PyTorch Lightning, and the dataset is wrapped in the `lightning.pytorch.LightningDataModule` class for seamless integration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chrombert.finetune.dataset.data_module.LitChromBERTFTDataModule at 0x7f6abdb92f80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module = chrombert.LitChromBERTFTDataModule(\n",
    "    config = dc,\n",
    "    train_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"train.csv\")),\n",
    "    val_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"valid.csv\")),\n",
    "    test_params = dict(supervised_file = os.path.join(\"tmp_ezh2\", \"test.csv\")),\n",
    ")\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the Model\n",
    "Next, we can instantiate the model using the ignore_index parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: mtx_mask = config/hg38_6k_mask_matrix.tsv\n",
      "update path: pretrain_ckpt = checkpoint/hg38_6k_1kb_pretrain.ckpt\n",
      "use organisim hg38; max sequence length is 6391\n",
      "Ignoring 203 cistromes and 1 regulators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTGeneral                                        --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               (4,916,736)\n",
       "â”‚    â””â”€ModuleList: 2-2                                  51,978,240\n",
       "â”œâ”€GeneralHeader: 1-2                                    --\n",
       "â”‚    â””â”€CistromeEmbeddingManager: 2-3                    --\n",
       "â”‚    â””â”€Conv2d: 2-4                                      769\n",
       "â”‚    â””â”€ReLU: 2-5                                        --\n",
       "â”‚    â””â”€ResidualBlock: 2-6                               3,249,152\n",
       "â”‚    â””â”€ResidualBlock: 2-7                               2,166,528\n",
       "â”‚    â””â”€ResidualBlock: 2-8                               460,032\n",
       "â”‚    â””â”€Linear: 2-9                                      257\n",
       "================================================================================\n",
       "Total params: 62,771,714\n",
       "Trainable params: 18,871,298\n",
       "Non-trainable params: 43,900,416\n",
       "================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = chrombert.get_preset_model_config(\n",
    "    \"general\", \n",
    "    ignore = True, ignore_index =ignore_index  # ignore_index from above\n",
    ").init_model()\n",
    "model.freeze_pretrain(trainable=2) # we only fine-tune the last two layers\n",
    "summary(model, depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune\n",
    "\n",
    "We fine-tune the model using PyTorch Lightning, employing a straightforward configuration to process parameters. The tuning is performed on a limited dataset to save time.\n",
    "\n",
    "Note: The tuning process involves randomness, so results may vary. For improved performance, consider increasing the number of epochs and expanding the size of the dataset used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chrombert.finetune.train.pl_module.ClassificationPLModule"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = chrombert.finetune.train.TrainConfig(\n",
    "    kind = \"classification\",\n",
    "    loss = \"bce\", # specify \"bce\" to use Binary Cross-Entropy (BCE) loss. Use \"focal\" to apply Focal Loss instead.\n",
    "    max_epochs = 1,\n",
    "    lr = 1e-4\n",
    ")\n",
    "pl_module = tc.init_pl_module(model) # wrap model with PyTorch Lightning module\n",
    "type(pl_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we begin the tuning process!  \n",
    "The trainer will save logs in a format compatible with TensorBoard, and multiple checkpoints may be generated during the process.  \n",
    "For this tutorial, however, we will use the latest model parameters instead of the checkpoints, as the tuning is insufficient.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-PCIE-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: tmp_ezh2/logs/ezh2\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | ChromBERTGeneral | 62.8 M\n",
      "-------------------------------------------\n",
      "18.9 M    Trainable params\n",
      "43.9 M    Non-trainable params\n",
      "62.8 M    Total params\n",
      "251.087   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b654285ff2884609baed5d0e0cdcab9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:480: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b94f8da7aaa44c5876702360626f361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818c7268f14b4f7ea57e1b4fdb24e095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3adbf6fcc4a45cfb0db359b77b45c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36beb7452f804706a8f5c08c0916bd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ad6ebd0c56425c93a1010d7774c112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e1d86fc544492885e9d53a6968aab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a835556e564920aec2854945f59520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f4df35a45b4aa9a07e75bf54af6b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e0ddd7b7e24730a6371d2e17de117c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\" # for tensorboard compatibility\n",
    "callback_ckpt = pl.callbacks.ModelCheckpoint( monitor = f\"{tc.tag}_validation/{tc.loss}\", mode = \"min\")\n",
    "# \n",
    "# \n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = tc.max_epochs,\n",
    "    accelerator = \"gpu\",\n",
    "    precision = \"bf16-mixed\",\n",
    "    fast_dev_run = False,\n",
    "    accumulate_grad_batches = 16, \n",
    "    logger = pl.loggers.TensorBoardLogger(os.path.join(\"tmp_ezh2\",\"logs\"), name = \"ezh2\"),\n",
    "    val_check_interval = 128,\n",
    "    limit_val_batches = 128,\n",
    "    log_every_n_steps = 1,\n",
    "    callbacks = [ callback_ckpt, pl.callbacks.LearningRateMonitor() ],\n",
    ")\n",
    "trainer.fit(pl_module, data_module)\n",
    "pl_module.save_ckpt(os.path.join(\"tmp_ezh2\", \"ezh2.ckpt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Use Fine-Tuned Model to Obtain Regulator Embeddings  \n",
    "\n",
    "ChromBERT has been successfully fine-tuned! You can directly access the tuned model using `pl_module.model`. However, due to specific settings in flash-attention, the dropout probability cannot be modified by `model.eval()`, which may introduce some randomness in the output.  \n",
    "\n",
    "To ensure consistent results, we recommend saving the checkpoint and loading it into the original model. This approach guarantees you are working with the fine-tuned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update path: mtx_mask = config/hg38_6k_mask_matrix.tsv\n",
      "update path: pretrain_ckpt = checkpoint/hg38_6k_1kb_pretrain.ckpt\n",
      "update path: finetune_ckpt = /home/yangdongxu/work/source/repos/ChromBERT/examples/tutorials/tmp_ezh2/ezh2.ckpt\n",
      "use organisim hg38; max sequence length is 6391\n",
      "Ignoring 203 cistromes and 1 regulators\n",
      "Loading checkpoint from /home/yangdongxu/work/source/repos/ChromBERT/examples/tutorials/tmp_ezh2/ezh2.ckpt\n",
      "Loaded 110/110 parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTGeneral                                        --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               4,916,736\n",
       "â”‚    â””â”€ModuleList: 2-2                                  51,978,240\n",
       "â”œâ”€GeneralHeader: 1-2                                    --\n",
       "â”‚    â””â”€CistromeEmbeddingManager: 2-3                    --\n",
       "â”‚    â””â”€Conv2d: 2-4                                      769\n",
       "â”‚    â””â”€ReLU: 2-5                                        --\n",
       "â”‚    â””â”€ResidualBlock: 2-6                               3,249,152\n",
       "â”‚    â””â”€ResidualBlock: 2-7                               2,166,528\n",
       "â”‚    â””â”€ResidualBlock: 2-8                               460,032\n",
       "â”‚    â””â”€Linear: 2-9                                      257\n",
       "================================================================================\n",
       "Total params: 62,771,714\n",
       "Trainable params: 62,771,714\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tuned = chrombert.get_preset_model_config(\n",
    "    \"general\", \n",
    "    dropout = 0,\n",
    "    ignore = True, ignore_index =ignore_index,   # ignore_index from above\n",
    "    finetune_ckpt = os.path.abspath(os.path.join(\"tmp_ezh2\", \"ezh2.ckpt\")) # use absolute path here\n",
    ").init_model()\n",
    "# or use model_tuned = pl_module.model\n",
    "summary(model_tuned, depth = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can get the embedding manager following the instruction of tutorials about extracting embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring 203 cistromes and 1 regulators\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "ChromBERTEmbedding                                      --\n",
       "â”œâ”€ChromBERT: 1-1                                        --\n",
       "â”‚    â””â”€BERTEmbedding: 2-1                               --\n",
       "â”‚    â”‚    â””â”€TokenEmbedding: 3-1                         7,680\n",
       "â”‚    â”‚    â””â”€PositionalEmbedding: 3-2                    4,909,056\n",
       "â”‚    â”‚    â””â”€Dropout: 3-3                                --\n",
       "â”‚    â””â”€ModuleList: 2-2                                  --\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-4                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-5                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-6                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-7                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-8                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-9                6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-10               6,497,280\n",
       "â”‚    â”‚    â””â”€EncoderTransformerBlock: 3-11               6,497,280\n",
       "â”œâ”€CistromeEmbeddingManager: 1-2                         --\n",
       "================================================================================\n",
       "Total params: 56,894,976\n",
       "Trainable params: 56,894,976\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_emb = model_tuned.get_embedding_manager().cuda()\n",
    "summary(model_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101,\n",
       " ['input_ids',\n",
       "  'position_ids',\n",
       "  'region',\n",
       "  'build_region_index',\n",
       "  'ignore_index',\n",
       "  'label'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc_test = data_module.test_config\n",
    "ds_test = dc_test.init_dataset()\n",
    "dl_test = dc_test.init_dataloader(batch_size = 1)\n",
    "len(ds_test), list(ds_test[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1101/1101 [01:28<00:00, 12.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573 528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1072, 768), (1072, 768))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain embeddings for both classical and non-classical EZH2 sites\n",
    "embs_classicial = []\n",
    "embs_nonclassicial = []\n",
    "for batch in tqdm(dl_test):\n",
    "    with torch.no_grad():\n",
    "        for k, v in batch.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                batch[k] = v.cuda()\n",
    "        emb = model_emb(batch)\n",
    "    if batch[\"label\"].item() == 1:\n",
    "        embs_classicial.append(emb)\n",
    "    else:\n",
    "        embs_nonclassicial.append(emb)\n",
    "\n",
    "print(len(embs_classicial), len(embs_nonclassicial))\n",
    "embs_classicial = torch.cat(embs_classicial, dim = 0).cpu().numpy().mean(axis = 0)\n",
    "embs_nonclassicial = torch.cat(embs_nonclassicial, dim = 0).cpu().numpy().mean(axis = 0)\n",
    "embs_classicial.shape, embs_nonclassicial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus exclusively on transcription factors, ignoring histone modifications and chromatin accessibility. This allows us to calculate the similarity between transcription factors, representing their potential interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['adnp', 'aebp2', 'aff1'], 991)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(os.path.join(basedir, \"config\",\"hg38_6k_factors_list.txt\"),\"r\") as f:\n",
    "    factors = f.read().strip().split(\"\\n\")\n",
    "factors = [f.strip().lower() for f in factors]\n",
    "factors[:3], len(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zzz3'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.in1d(model_emb.list_regulator,factors)\n",
    "names = np.array(model_emb.list_regulator)[indices]\n",
    "embs_classicial = embs_classicial[indices]\n",
    "embs_nonclassicial = embs_nonclassicial[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_classicial_matrix = cosine_similarity(embs_classicial)\n",
    "cos_nonclassicial_matrix = cosine_similarity(embs_nonclassicial)\n",
    "df_cos_classicial = pd.DataFrame(cos_classicial_matrix, columns = names, index = names)\n",
    "df_cos_nonclassicial = pd.DataFrame(cos_nonclassicial_matrix, columns = names, index = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5776264667510986, 0.5518490076065063)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we define threshold to select the most related regulators pairs\n",
    "thre_class = np.percentile(cos_classicial_matrix.flatten(), 95)\n",
    "thre_nonclass = np.percentile(cos_nonclassicial_matrix.flatten(), 95)\n",
    "thre_class, thre_nonclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we identify TRNs associated with the non-classical functions of EZH2. As you can see, factors related to the classical functions of EZH2 are associated with the Polycomb complex, such as SUZ12. In contrast, factors linked to EZH2's non-classical functions tend to be associated with transcriptional activation, including EP300 and STAT3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adnp</th>\n",
       "      <td>0.193229</td>\n",
       "      <td>0.359759</td>\n",
       "      <td>-0.166530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aebp2</th>\n",
       "      <td>0.190192</td>\n",
       "      <td>0.309486</td>\n",
       "      <td>-0.119294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aff1</th>\n",
       "      <td>0.396427</td>\n",
       "      <td>0.479881</td>\n",
       "      <td>-0.083454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aff4</th>\n",
       "      <td>0.300634</td>\n",
       "      <td>0.452184</td>\n",
       "      <td>-0.151550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago1</th>\n",
       "      <td>0.234017</td>\n",
       "      <td>0.291643</td>\n",
       "      <td>-0.057626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zscan5a</th>\n",
       "      <td>0.218051</td>\n",
       "      <td>0.334104</td>\n",
       "      <td>-0.116052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zta</th>\n",
       "      <td>0.181115</td>\n",
       "      <td>0.251131</td>\n",
       "      <td>-0.070015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zxdb</th>\n",
       "      <td>0.213376</td>\n",
       "      <td>0.294275</td>\n",
       "      <td>-0.080898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zxdc</th>\n",
       "      <td>0.149416</td>\n",
       "      <td>0.287630</td>\n",
       "      <td>-0.138214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zzz3</th>\n",
       "      <td>0.248419</td>\n",
       "      <td>0.346867</td>\n",
       "      <td>-0.098448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         classical  nonclassical      diff\n",
       "adnp      0.193229      0.359759 -0.166530\n",
       "aebp2     0.190192      0.309486 -0.119294\n",
       "aff1      0.396427      0.479881 -0.083454\n",
       "aff4      0.300634      0.452184 -0.151550\n",
       "ago1      0.234017      0.291643 -0.057626\n",
       "...            ...           ...       ...\n",
       "zscan5a   0.218051      0.334104 -0.116052\n",
       "zta       0.181115      0.251131 -0.070015\n",
       "zxdb      0.213376      0.294275 -0.080898\n",
       "zxdc      0.149416      0.287630 -0.138214\n",
       "zzz3      0.248419      0.346867 -0.098448\n",
       "\n",
       "[991 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2 = pd.DataFrame(index =names, data = {\"classical\":df_cos_classicial.loc[\"ezh2\",:],\"nonclassical\":df_cos_nonclassicial.loc[\"ezh2\",:]})\n",
    "df_cos_ezh2[\"diff\"] = df_cos_ezh2[\"classical\"] - df_cos_ezh2[\"nonclassical\"]\n",
    "df_cos_ezh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ezh1</th>\n",
       "      <td>0.644102</td>\n",
       "      <td>0.578448</td>\n",
       "      <td>6.565380e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pcgf1</th>\n",
       "      <td>0.627553</td>\n",
       "      <td>0.564524</td>\n",
       "      <td>6.302953e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kdm2b</th>\n",
       "      <td>0.600312</td>\n",
       "      <td>0.543989</td>\n",
       "      <td>5.632281e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jarid2</th>\n",
       "      <td>0.660526</td>\n",
       "      <td>0.609975</td>\n",
       "      <td>5.055112e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rybp</th>\n",
       "      <td>0.594199</td>\n",
       "      <td>0.549152</td>\n",
       "      <td>4.504716e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suz12</th>\n",
       "      <td>0.875248</td>\n",
       "      <td>0.833695</td>\n",
       "      <td>4.155296e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bcor</th>\n",
       "      <td>0.616664</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>2.887654e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eed</th>\n",
       "      <td>0.597332</td>\n",
       "      <td>0.576189</td>\n",
       "      <td>2.114320e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ezh2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-3.576279e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbx8</th>\n",
       "      <td>0.584771</td>\n",
       "      <td>0.621309</td>\n",
       "      <td>-3.653848e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        classical  nonclassical          diff\n",
       "ezh1     0.644102      0.578448  6.565380e-02\n",
       "pcgf1    0.627553      0.564524  6.302953e-02\n",
       "kdm2b    0.600312      0.543989  5.632281e-02\n",
       "jarid2   0.660526      0.609975  5.055112e-02\n",
       "rybp     0.594199      0.549152  4.504716e-02\n",
       "suz12    0.875248      0.833695  4.155296e-02\n",
       "bcor     0.616664      0.587787  2.887654e-02\n",
       "eed      0.597332      0.576189  2.114320e-02\n",
       "ezh2     1.000000      1.000000 -3.576279e-07\n",
       "cbx8     0.584771      0.621309 -3.653848e-02"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2.query(\"classical > @thre_class \").sort_values(\"diff\", ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classical</th>\n",
       "      <th>nonclassical</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>foxm1</th>\n",
       "      <td>0.395081</td>\n",
       "      <td>0.604784</td>\n",
       "      <td>-0.209703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>med1</th>\n",
       "      <td>0.349747</td>\n",
       "      <td>0.556351</td>\n",
       "      <td>-0.206604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat3</th>\n",
       "      <td>0.424211</td>\n",
       "      <td>0.614199</td>\n",
       "      <td>-0.189988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep300</th>\n",
       "      <td>0.411506</td>\n",
       "      <td>0.599940</td>\n",
       "      <td>-0.188434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hinfp</th>\n",
       "      <td>0.391241</td>\n",
       "      <td>0.577772</td>\n",
       "      <td>-0.186532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rela</th>\n",
       "      <td>0.385995</td>\n",
       "      <td>0.568461</td>\n",
       "      <td>-0.182466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smarca4</th>\n",
       "      <td>0.383161</td>\n",
       "      <td>0.552061</td>\n",
       "      <td>-0.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brca1</th>\n",
       "      <td>0.415508</td>\n",
       "      <td>0.583485</td>\n",
       "      <td>-0.167977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stat1</th>\n",
       "      <td>0.431552</td>\n",
       "      <td>0.594642</td>\n",
       "      <td>-0.163090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e2f4</th>\n",
       "      <td>0.423399</td>\n",
       "      <td>0.568790</td>\n",
       "      <td>-0.145391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         classical  nonclassical      diff\n",
       "foxm1     0.395081      0.604784 -0.209703\n",
       "med1      0.349747      0.556351 -0.206604\n",
       "stat3     0.424211      0.614199 -0.189988\n",
       "ep300     0.411506      0.599940 -0.188434\n",
       "hinfp     0.391241      0.577772 -0.186532\n",
       "rela      0.385995      0.568461 -0.182466\n",
       "smarca4   0.383161      0.552061 -0.168900\n",
       "brca1     0.415508      0.583485 -0.167977\n",
       "stat1     0.431552      0.594642 -0.163090\n",
       "e2f4      0.423399      0.568790 -0.145391"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_ezh2.query(\"nonclassical > @thre_nonclass \").sort_values(\"diff\", ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "This tutorial offers a comprehensive guide to context-specific TRN inference, using EZH2's functional collaborations as an example.  \n",
    "We hope you find it both helpful and informative.  \n",
    "If you have any questions or require further assistance, please don't hesitate to reach out. Thank you for following along!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
